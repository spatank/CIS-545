{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_2_SPP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W0hcZWDcqCUL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-545/blob/master/Homework_2_SPP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlv8esDGMj0h"
      },
      "source": [
        "# CIS 545 Homework 2\n",
        "## Due 12 October, 2020 by 10pm Eastern Time\n",
        "### Worth 100 points in total\n",
        "\n",
        "Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment will focus on broadening both of these horizons by covering hierarchical data, graphs, and traversing relationships as well as two new tools: SQL and Spark. \n",
        "\n",
        "In the first section, we will familiarize ourselves with SQL (specifically **pandassql** and explore the Stack Exchange dataset. We will also finish out the section with some text analysis.\n",
        "\n",
        "The second section will focus on graph data and give you a small preview of Spark using the Yelp dataset. This homework is designed to introduce you to Spark's required workflow before you fully unlease its power next homework and deploy it on an AWS cluster. \n",
        "\n",
        "We are introducing a lot of new things in this homework, and it is often where students start to get lost in the data science sauce, so we **strongly** encourage you to review the slides/material as you work through this assignment and will try to link the most relevant sections!\n",
        "\n",
        "**Before you Begin**\n",
        "- Be sure to click \"Copy to Drive\" to make sure you are working on your own personal version of the homework\n",
        "- Read the Piazza and FAQ for updates! If you have been stuck, chances are other students are too! We don't want you to waste away for two hours trying to get that last point on the autograder so do check Piazza for similar struggles or even homework bugs that will be clarified in the FAQ :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XoSSg8VDX_4"
      },
      "source": [
        "# Section 0: Homework Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjsiej7E_zx2"
      },
      "source": [
        "## Part -2: Install the Proper Version of Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n8oxVzQ_qno"
      },
      "source": [
        "Run the following cell to install the proper version of pandas. After running this cell, restart your runtime (Runtime > Restart runtime) and then run all the remaining set up cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MsEmBo3_n6O",
        "outputId": "d862bc5f-bb0f-4c6d-fabf-713eebb501e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip3 install pandas==1.0.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas==1.0.5 in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.5) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.5) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7kVh6j17JyC",
        "outputId": "4025bad2-918b-4a41-e2c5-eca007e06013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# make sure that this cell prints True! Otherwise you may have forgotten to\n",
        "# restart your runtime after running the cell above\n",
        "import pandas as pd\n",
        "print(pd.__version__ == '1.0.5')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyWoMn6pxSC"
      },
      "source": [
        "## Part -1: Enter your PennID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ds2HHSkpvBO"
      },
      "source": [
        "STUDENT_ID =  22993135"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0hcZWDcqCUL"
      },
      "source": [
        "## Part 0: Libraries and Set Up Jargon (The usual wall of imports)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYJJdBQEcg1"
      },
      "source": [
        "#! sudo apt install openjdk-8-jdk\n",
        "#! sudo update-alternatives --config java"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1C8EIvyEq1S",
        "outputId": "63864637-02b9-481f-9bf5-5fe470854e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip3 install penngrader\n",
        "\n",
        "from penngrader.grader import *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: penngrader in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTE9TG8Aqaz8",
        "outputId": "e26f4dbd-552a-4b1f-f449-89ea6dbc10f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS545_Fall_2020_HW2', student_id = STUDENT_ID)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PennGrader initialized with Student ID: 22993135\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiOqbuZCF-EL"
      },
      "source": [
        "### Install required packages\n",
        "%%capture\n",
        "!pip3 install lxml\n",
        "!pip install pandasql\n",
        "!pip install googledrivedownloader"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zkXvBH-F-l4",
        "outputId": "070df61d-d3b9-43da-b0bc-0a7ea774b190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import json # JSON parsing\n",
        "from lxml import etree # HTML parsing\n",
        "import time # Time conversions\n",
        "from lxml import etree # XML Parser\n",
        "import pandasql as ps # SQL on Pandas Dataframe\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzZ6JNnKLplQ"
      },
      "source": [
        "\n",
        "\n",
        "# Section 1: Exploring the Stack Exchange Dataset\n",
        "\n",
        "\n",
        "<img src = \"https://cdn.sstatic.net/Sites/stackoverflow/company/img/logos/se/se-logo.png?v=dd7153fcc7fa\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "To survive as a student at Penn , you've certainly used Stack Exchange or Stack Overflow, as a source for all your technical queries. Stack Exchange looks a lot like a social network, it has the following pieces of information to tie it all together:\n",
        "\n",
        "\n",
        "*   Users: All stack exchange users including admins etc.\n",
        "\n",
        "*   Posts: All the questions as well as the answers that users post\n",
        "\n",
        "*   Comments: As the name suggests, these are comments on posts\n",
        "\n",
        "*   Votes: Up/Downvotes \n",
        "\n",
        "\n",
        "For this homework we'll be parsing this data (dumped in XML) into dataframes and relations, and then exploring how to query and assemble the tables into results with Pandas and  PandaSQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENmSeFkFRCo"
      },
      "source": [
        "## Part 1: Loading our datasets [12 points total]\n",
        "\n",
        "Before we get into the data, we first need to load our datasets. We will actually only be using the Users and Posts datasets for our queries, but we want you to write a generalized xml parsing function that would be able to convert any of the xml files into a dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aAGG11y8I4r"
      },
      "source": [
        "### 1.0 Importing Data\n",
        "\n",
        "Below is the code to import the xml files from our shared Google Drive. The data is relatively small, so this shouldn't take too long. We will only import the Users and Posts xmls for now, but the other datasets are there in case you want to take a look :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3G_ZtXFpH_"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1T-SGFULQIkpg6LN5XLhBfiXhcAWNgCLe',\n",
        "                                    dest_path='/content/Users.xml')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1etuY-EjzgEfMdPCSd7NblNz0qJJpAH3b',\n",
        "                                    dest_path='/content/Posts.xml')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX6NErPmFwfu"
      },
      "source": [
        "### 1.1 Load Dataset Function\n",
        "\n",
        "Now that we finally have all our packages imported and datasets initalized, it's time to finally write some code! Your first task is to write the function **xml_to_df(file_path)** that will parse the specified file into a dataframe. This function should be generalized, in the sense that it can accept any of the xml files that we loaded and return a dataframe. We highly recommend looking over the [xml documentation](https://docs.python.org/2/library/xml.etree.elementtree.html) in order to accomplish this task.\n",
        "\n",
        "**TODO:** Once you have written **xml_to_df(file_path)**, create a **posts_df** and **users_df** with the parsed XML files (`/content/Users.xml` and `/content/Posts.xml`)\n",
        "\n",
        "Tip: try figuring out the steps with one of the two XML files first! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aYnSb9yJBdr",
        "outputId": "ac8cdbf3-93ae-4ff3-fadb-301846d283ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Posts.xml  sample_data\tUsers.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJx3M62XHS5q"
      },
      "source": [
        "def xml_to_df(file_path):\n",
        "  \"\"\" Converts an xml file to a dataframe\n",
        "\n",
        "  :param file_path: path to file\n",
        "  :return: dataframe \n",
        "  \"\"\"\n",
        "  \n",
        "  tree = etree.ElementTree()\n",
        "  tree.parse(file_path)\n",
        "  root = tree.getroot()\n",
        "  lst = []\n",
        "  for child in root:\n",
        "    child_dict = dict(child.attrib)\n",
        "    lst.append(child_dict)\n",
        "\n",
        "  df = pd.DataFrame(lst)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WxkXoLzGHvz"
      },
      "source": [
        "posts_df = xml_to_df('Posts.xml')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ7N7VCjoi-i",
        "outputId": "a37d313b-a57d-4331-a0a4-25b2ff6b4f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# [CIS 545 PennGrader Cell] - 5 points\n",
        "grader.grade(test_case_id = 'test_xml_to_posts_df', answer = posts_df[:75])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBV1ZMjzS4ja"
      },
      "source": [
        "users_df = xml_to_df('Users.xml')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmpIsMjcS5wD",
        "outputId": "7a8281d8-3c8b-495b-9b4e-361de4955218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_xml_to_users_df', answer = users_df[:75])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPitr7eRJfO0"
      },
      "source": [
        "### 1.2 Clean Dataset\n",
        "\n",
        "Next, we are going to want to clean up our dataframes, namely 1) removing null values,  2) changing datatypes, and 3) dropping columns\n",
        "\n",
        "Originally, we were going to have you identify the datatypes with this [image](https://i.stack.imgur.com/AyIkW.png) on your own, but I (the TA writing this section) found this part really tedious and rage-inducing so we have defined the specific columns to convert below. All you need to do is write the function :)\n",
        "\n",
        "**TODO**: 1) replace all null values in both datasets **in-place**. 2) define a function **dtype_converter(df, int_columns)** that takes in a dateframe and a list specifying which columns should be integers. Then, use this function on both posts_df and users_df using the lists defined below. (Note: we don't need to convert any columns to strings since they're already objects, and we're ignoring datetime)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlWGmIvB-qtl"
      },
      "source": [
        "posts_df.fillna(0, inplace = True)\n",
        "users_df.fillna(0, inplace = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptWgCcPeGWHt"
      },
      "source": [
        "# columns that need to be integers\n",
        "\n",
        "int_posts_cols = [\"Id\", \"PostTypeId\", \"AcceptedAnswerId\", \"ParentId\", \"Score\", \n",
        "                  \"ViewCount\", \"OwnerUserId\", \"LastEditorUserId\", \"AnswerCount\",\n",
        "                  \"CommentCount\", \"FavoriteCount\"]\n",
        "int_users_cols = [\"Id\", \"Reputation\", \"Views\", \"UpVotes\", \"DownVotes\", \"AccountId\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6uZaW_GJ7_s"
      },
      "source": [
        "def dtype_converter(df, int_columns):\n",
        "  \"\"\"converts columns to type integer\n",
        "\n",
        "  :param df: dataframe to convert\n",
        "  :param int_columns: list of columns to convert\n",
        "  :return: dataframe\n",
        "  \"\"\"\n",
        "  change_dict = {}\n",
        "  for key in int_columns:\n",
        "    change_dict[key] = int\n",
        "  \n",
        "  df = df.astype(change_dict)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub5pF19jKn_X"
      },
      "source": [
        "posts_df = dtype_converter(posts_df, int_posts_cols) \n",
        "users_df = dtype_converter(users_df, int_users_cols) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOsPH8RcWNoP",
        "outputId": "f26ed512-4168-4a12-e9e2-70ed3421abe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# check your datatypes\n",
        "posts_df.dtypes "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                        int64\n",
              "PostTypeId                int64\n",
              "CreationDate             object\n",
              "Score                     int64\n",
              "ViewCount                 int64\n",
              "Body                     object\n",
              "OwnerUserId               int64\n",
              "LastActivityDate         object\n",
              "Title                    object\n",
              "Tags                     object\n",
              "AnswerCount               int64\n",
              "CommentCount              int64\n",
              "FavoriteCount             int64\n",
              "ClosedDate               object\n",
              "AcceptedAnswerId          int64\n",
              "LastEditorUserId          int64\n",
              "LastEditDate             object\n",
              "ParentId                  int64\n",
              "OwnerDisplayName         object\n",
              "CommunityOwnedDate       object\n",
              "LastEditorDisplayName    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngIbo_vVK5a",
        "outputId": "c71e43a3-56d1-465d-c324-b050700301a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_posts_dtypes', answer = posts_df[:75])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WJkrBGVHGg",
        "outputId": "5aa6e7ae-a8f1-4f0e-bccb-2f9e098a2831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_users_dtypes', answer = users_df[:75])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYp9fW_SvG3g"
      },
      "source": [
        "## Part 1.5 Your Sandbox \n",
        "\n",
        "Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just jump in, but we wanted to at least give you a small space to utilize your basic EDA toolkit to familiarize yourself with all the info you just downloaded.\n",
        "\n",
        "Some suggestions to get you started:\n",
        "- `df.head()`\n",
        "- `df.info()`\n",
        "- `df.describe()`\n",
        "\n",
        "Also, definitely take a look at [this readme](https://ia800107.us.archive.org/27/items/stackexchange/readme.txt) that provides a good overview of all the datasets (ignore the ones that you did not ask you to convert)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYbq6dN5snWs"
      },
      "source": [
        "# your EDA here! feel free to add more cells"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFdRtQKLbti"
      },
      "source": [
        "## Part 2: Exploring the data with Pandas and PandasSQL [20 points total]\n",
        "\n",
        "Now that you are familiar (or still unfamiliar) with the dataset, we will now introduce you to SQL, or more specifically **pandasql**: a package create to allow users to query pandas DataFrames with SQL statements.\n",
        "\n",
        "The typical flow to use pandasql (shortened to **ps**) is as follows:\n",
        "1. write a SQL query in the form of a string (Tip: use triple quotes \"\"\"x\"\"\" to write multi-line strings)\n",
        "2. run the query using **ps.sqldf(your_query, locals())**\n",
        "\n",
        "Pandasql is convenient in that it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the `posts_df` and `users_df` that you have created above!\n",
        "\n",
        "Given that it is a brand new language, we wanted to give you a chance to directly compare the similarities/differences of the pandas that you already know and the SQL you are about to learn. Thus, for each query, we ask that you to **look into the question twice: once with pandas and once with pandasql**. \n",
        "\n",
        "Each answer will thus require both a `pd_` and `sql_` prefixed-dataframe that you will submit seperately to the autograder. **We will be reviewing your code to make sure you wrote the code in the corresponding languages.**\n",
        "\n",
        " [Here](https://community.alteryx.com/t5/Data-Science/pandasql-Make-python-speak-SQL/ba-p/138435) is a good resource to review pandasql. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geYyH57csade"
      },
      "source": [
        "### 2.1 Spliting Up `posts_df`\n",
        "\n",
        "`posts_df` actually contains both posted questions and the answers. The provided readme details the distinguishing factors as follows:\n",
        "\n",
        "        - PostTypeId\n",
        "            - 1: Question\n",
        "            - 2: Answer\n",
        "        - ParentID (only present if PostTypeId is 2)\n",
        "        - AcceptedAnswerId (only present if PostTypeId is 1)\n",
        "\n",
        "**TODO:** Using pandas/pandasql, split `posts_df` into a `pd/sql_questions_df` and `pd/sql_answers_df` based on these values of `PostTypeId`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laL9vimDbPYQ"
      },
      "source": [
        "pd_questions_df = posts_df[posts_df['PostTypeId'] == 1] \n",
        "pd_answers_df = posts_df[posts_df['PostTypeId'] == 2] "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQjSBREeNQL",
        "outputId": "ca580843-7561-4243-ac8d-af59ae6ed427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_questions_df', answer = pd_questions_df[\"PostTypeId\"].values)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tGZjFP-gdVo",
        "outputId": "556fcee2-fdd1-4ced-d065-7fa816b92a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_answers_df', answer = pd_answers_df[\"PostTypeId\"].values)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOIT18_uqJ8c"
      },
      "source": [
        "questions_query = \"SELECT * FROM posts_df WHERE PostTypeId = 1\"\n",
        "answers_query = \"SELECT * FROM posts_df WHERE PostTypeId = 2\"\n",
        "\n",
        "sql_questions_df = ps.sqldf(questions_query)\n",
        "sql_answers_df = ps.sqldf(answers_query)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqtjXDPeNjd",
        "outputId": "e98dfe78-fa7b-44e2-9a2e-909e15caceb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_qa_query', answer = (questions_query, answers_query))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_DeMOYjldH"
      },
      "source": [
        "# using just our sql dataframe moving forward\n",
        "questions_df = sql_questions_df\n",
        "answers_df = sql_answers_df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooVqjTGm9eCA"
      },
      "source": [
        "### 2.2 What are the most popular questions?\n",
        "\n",
        "**TODO**: Use `questions_df` to find the 10 most popular questions by `ViewCount`.\n",
        "\n",
        "Store the results in `pd/sql_popular_df` which be have the following format:\n",
        "\n",
        ">Id | Title | ViewCount\n",
        ">--- | --- | ---\n",
        "\n",
        "Hint: for your SQL query, you will need to know `ORDER BY`, `LIMIT`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scVrGa4CjS08",
        "outputId": "3a1f07fa-7765-468f-ef58-829f567526b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "pd_popular_df = questions_df[['Id', 'Title', 'ViewCount']].sort_values(\\\n",
        "                                                                       by = 'ViewCount', ascending = False).iloc[:10]\n",
        "pd_popular_df"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Title</th>\n",
              "      <th>ViewCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>893</td>\n",
              "      <td>How to get correlation between two categorical...</td>\n",
              "      <td>202040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>969</th>\n",
              "      <td>6107</td>\n",
              "      <td>What are deconvolutional layers?</td>\n",
              "      <td>180519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3094</th>\n",
              "      <td>13490</td>\n",
              "      <td>How to set class weights for imbalanced classe...</td>\n",
              "      <td>167923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22</td>\n",
              "      <td>K-Means clustering for mixed numeric and categ...</td>\n",
              "      <td>167107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2051</th>\n",
              "      <td>10459</td>\n",
              "      <td>Calculation and Visualization of Correlation M...</td>\n",
              "      <td>160305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2705</th>\n",
              "      <td>12321</td>\n",
              "      <td>Difference between fit and fit_transform in sc...</td>\n",
              "      <td>158049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>11928</td>\n",
              "      <td>ValueError: Input contains NaN, infinity or a ...</td>\n",
              "      <td>143746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9254</th>\n",
              "      <td>33053</td>\n",
              "      <td>How do I compare columns in different data fra...</td>\n",
              "      <td>139491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1626</th>\n",
              "      <td>9302</td>\n",
              "      <td>The cross-entropy error function in neural net...</td>\n",
              "      <td>134651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3868</th>\n",
              "      <td>15989</td>\n",
              "      <td>Micro Average vs Macro average Performance in ...</td>\n",
              "      <td>115817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Id                                              Title  ViewCount\n",
              "235     893  How to get correlation between two categorical...     202040\n",
              "969    6107                   What are deconvolutional layers?     180519\n",
              "3094  13490  How to set class weights for imbalanced classe...     167923\n",
              "7        22  K-Means clustering for mixed numeric and categ...     167107\n",
              "2051  10459  Calculation and Visualization of Correlation M...     160305\n",
              "2705  12321  Difference between fit and fit_transform in sc...     158049\n",
              "2563  11928  ValueError: Input contains NaN, infinity or a ...     143746\n",
              "9254  33053  How do I compare columns in different data fra...     139491\n",
              "1626   9302  The cross-entropy error function in neural net...     134651\n",
              "3868  15989  Micro Average vs Macro average Performance in ...     115817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_hw6BLSj3-p",
        "outputId": "00b8a807-d0fe-4d94-9945-08b6a9819e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_popular_df', answer = pd_popular_df)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK2IfiYa-FKL",
        "outputId": "8917be1c-58ae-4473-964b-7896e0b0c72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "popular_query = 'SELECT Id, Title, ViewCount FROM questions_df ORDER BY ViewCount DESC LIMIT 10'\n",
        "sql_popular_df = ps.sqldf(popular_query)\n",
        "sql_popular_df"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Title</th>\n",
              "      <th>ViewCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>893</td>\n",
              "      <td>How to get correlation between two categorical...</td>\n",
              "      <td>202040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6107</td>\n",
              "      <td>What are deconvolutional layers?</td>\n",
              "      <td>180519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13490</td>\n",
              "      <td>How to set class weights for imbalanced classe...</td>\n",
              "      <td>167923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22</td>\n",
              "      <td>K-Means clustering for mixed numeric and categ...</td>\n",
              "      <td>167107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10459</td>\n",
              "      <td>Calculation and Visualization of Correlation M...</td>\n",
              "      <td>160305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12321</td>\n",
              "      <td>Difference between fit and fit_transform in sc...</td>\n",
              "      <td>158049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>11928</td>\n",
              "      <td>ValueError: Input contains NaN, infinity or a ...</td>\n",
              "      <td>143746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33053</td>\n",
              "      <td>How do I compare columns in different data fra...</td>\n",
              "      <td>139491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9302</td>\n",
              "      <td>The cross-entropy error function in neural net...</td>\n",
              "      <td>134651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15989</td>\n",
              "      <td>Micro Average vs Macro average Performance in ...</td>\n",
              "      <td>115817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id                                              Title  ViewCount\n",
              "0    893  How to get correlation between two categorical...     202040\n",
              "1   6107                   What are deconvolutional layers?     180519\n",
              "2  13490  How to set class weights for imbalanced classe...     167923\n",
              "3     22  K-Means clustering for mixed numeric and categ...     167107\n",
              "4  10459  Calculation and Visualization of Correlation M...     160305\n",
              "5  12321  Difference between fit and fit_transform in sc...     158049\n",
              "6  11928  ValueError: Input contains NaN, infinity or a ...     143746\n",
              "7  33053  How do I compare columns in different data fra...     139491\n",
              "8   9302  The cross-entropy error function in neural net...     134651\n",
              "9  15989  Micro Average vs Macro average Performance in ...     115817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzgw1okj7Ju",
        "outputId": "197a13e5-6aef-4308-9e52-7fbbbdd0fe7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_popular_query', answer = popular_query)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcJvhlRfkTgt",
        "outputId": "a3e246eb-5970-4ab5-92a8-3a328626788a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_sql_popular_df', answer = sql_popular_df)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5L0XqjBLu_"
      },
      "source": [
        "### 2.3 Who are the most helpful users?\n",
        "\n",
        "**TODO:**  Use `answers_df` to find the names of the top 10 users who answer the most questions on stack exchange. This should be based on the count of unique answers made by the user.\n",
        "\n",
        "Your answer, stored in `pd/sql_talkative_df` will have the following format:\n",
        "\n",
        ">UserId | DisplayName | ResponseCount\n",
        ">--- | --- | ---\n",
        "\n",
        "\n",
        "Note: both `users_df` and `answers_df` have an `Id` column, but store entirely different values in them! \n",
        "\n",
        "SQL Hint: The tools that you will need include, but are not limited to`AS`, `JOIN`, `GROUP BY`, `ORDER BY` and `LIMIT`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lC-kw648e_d"
      },
      "source": [
        "users_answers_df = users_df.merge(answers_df, left_on = 'Id', right_on = 'OwnerUserId')\n",
        "answers_by_users = users_answers_df.groupby(['Id_x', 'DisplayName']).size().sort_values(ascending = False).iloc[:10].to_frame()\n",
        "answers_by_users.reset_index(inplace = True)\n",
        "mapping = {answers_by_users.columns[0]: 'UserId', answers_by_users.columns[2]: 'ResponseCount'} \n",
        "answers_by_users.rename(columns = mapping, inplace = True)\n",
        "pd_talkative_df = answers_by_users"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jSUCGU5kWJ7",
        "outputId": "92280f00-59b1-4b98-8d9f-62af27ba6dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "pd_talkative_df"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>DisplayName</th>\n",
              "      <th>ResponseCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>836</td>\n",
              "      <td>Neil Slater</td>\n",
              "      <td>408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45264</td>\n",
              "      <td>n1k31t4</td>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>924</td>\n",
              "      <td>Anony-Mousse</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28175</td>\n",
              "      <td>Media</td>\n",
              "      <td>330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1330</td>\n",
              "      <td>Brian Spiering</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>381</td>\n",
              "      <td>Emre</td>\n",
              "      <td>227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64377</td>\n",
              "      <td>Erwan</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>29587</td>\n",
              "      <td>JahKnows</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14904</td>\n",
              "      <td>Jan van der Vegt</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>67328</td>\n",
              "      <td>Esmailian</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserId       DisplayName  ResponseCount\n",
              "0     836       Neil Slater            408\n",
              "1   45264           n1k31t4            363\n",
              "2     924      Anony-Mousse            345\n",
              "3   28175             Media            330\n",
              "4    1330    Brian Spiering            270\n",
              "5     381              Emre            227\n",
              "6   64377             Erwan            221\n",
              "7   29587          JahKnows            203\n",
              "8   14904  Jan van der Vegt            184\n",
              "9   67328         Esmailian            152"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xZO_F9D06E8",
        "outputId": "4a53c7c2-1605-44e1-ce36-b604765d9490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_talkative_df', answer = pd_talkative_df)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpQevBFf9DUL",
        "outputId": "5e916f67-b298-43f5-c358-38a942faae2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "talkative_query = \\\n",
        "'''\n",
        "SELECT users_df.Id AS UserId, \n",
        "users_df.DisplayName AS DisplayName,\n",
        "COUNT(users_df.Id) AS ResponseCount\n",
        "FROM users_df INNER JOIN answers_df ON users_df.Id = answers_df.OwnerUserId \n",
        "GROUP BY users_df.Id \n",
        "ORDER BY COUNT(users_df.Id) DESC LIMIT 10\n",
        "'''\n",
        "sql_talkative_df = ps.sqldf(talkative_query)\n",
        "sql_talkative_df"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>DisplayName</th>\n",
              "      <th>ResponseCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>836</td>\n",
              "      <td>Neil Slater</td>\n",
              "      <td>408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45264</td>\n",
              "      <td>n1k31t4</td>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>924</td>\n",
              "      <td>Anony-Mousse</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28175</td>\n",
              "      <td>Media</td>\n",
              "      <td>330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1330</td>\n",
              "      <td>Brian Spiering</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>381</td>\n",
              "      <td>Emre</td>\n",
              "      <td>227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64377</td>\n",
              "      <td>Erwan</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>29587</td>\n",
              "      <td>JahKnows</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14904</td>\n",
              "      <td>Jan van der Vegt</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>67328</td>\n",
              "      <td>Esmailian</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserId       DisplayName  ResponseCount\n",
              "0     836       Neil Slater            408\n",
              "1   45264           n1k31t4            363\n",
              "2     924      Anony-Mousse            345\n",
              "3   28175             Media            330\n",
              "4    1330    Brian Spiering            270\n",
              "5     381              Emre            227\n",
              "6   64377             Erwan            221\n",
              "7   29587          JahKnows            203\n",
              "8   14904  Jan van der Vegt            184\n",
              "9   67328         Esmailian            152"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhGqClcm1KRO",
        "outputId": "beace522-4c9b-4440-eff7-51df9e6c22a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_talkative_query', answer = talkative_query)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oidAVJro2Eda",
        "outputId": "2f64ffc9-b6b7-4176-cb59-cac020930d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_sql_talkative_df', answer = sql_talkative_df)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdwzQ2XtF2ki"
      },
      "source": [
        "### 2.4 Who are the most helpful-in-a-different-kind-of-way users?\n",
        "\n",
        "**TODO**: find the users that ask a lot of questions, but have never posted an answer. To accomplish this, you are going to want to find all the users in `questions_df` that don't appear in `answers_df`. Sort by `QuestionsCount` descending and store only the top 5 results.\n",
        "\n",
        "The query will require you to write a [nested SQL query](https://learnsql.com/blog/sql-nested-select/). That is, there will be at least one select statement inside of a select statement. This means that you **should NOT** write two seperate SQL commands and call ps.sqldf() twice. \n",
        "\n",
        "Though it would be helpful, **you do NOT have to implement this in pandas**. Your answer, stored in `askers_df` will have the following format:\n",
        "\n",
        ">UserId | DisplayName | QuestionsCount\n",
        ">--- | --- | ---\n",
        "\n",
        "\n",
        "SQL Hint: You can use `NOT IN` or `LEFT JOIN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baFcGJa-rikA",
        "outputId": "79306ec5-ecd8-4bed-8dd1-edf54ef9d781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "askers_query = \\\n",
        "'''\n",
        "SELECT users_df.Id AS UserId,\n",
        "users_df.DisplayName as DisplayName,\n",
        "COUNT(users_df.Id) AS QuestionsCount\n",
        "FROM users_df INNER JOIN\n",
        "(\n",
        "  SELECT *\n",
        "  FROM questions_df\n",
        "  WHERE questions_df.OwnerUserId NOT IN\n",
        "  (SELECT answers_df.OwnerUserId FROM answers_df)\n",
        ") q_not_a\n",
        "ON users_df.Id = q_not_a.OwnerUserId\n",
        "GROUP BY users_df.Id \n",
        "ORDER BY COUNT(users_df.Id) DESC LIMIT 5\n",
        "'''\n",
        "\n",
        "askers_df = ps.sqldf(askers_query)\n",
        "askers_df"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>DisplayName</th>\n",
              "      <th>QuestionsCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17310</td>\n",
              "      <td>Edamame</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51129</td>\n",
              "      <td>N.IT</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8013</td>\n",
              "      <td>girl101</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58433</td>\n",
              "      <td>user10296606</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7812</td>\n",
              "      <td>william007</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserId   DisplayName  QuestionsCount\n",
              "0   17310       Edamame              42\n",
              "1   51129          N.IT              38\n",
              "2    8013       girl101              36\n",
              "3   58433  user10296606              32\n",
              "4    7812    william007              29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3CfB-VVrGR",
        "outputId": "c1d370d1-a8bc-4f60-a87b-870d220e6c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_askers_query', answer = askers_query)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S96GfLS8Vzn9",
        "outputId": "ecc91835-cc22-45e5-9afb-0bc08836dc5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_askers_df', answer = askers_df)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmzgmtU2USvQ"
      },
      "source": [
        "### 2.5 So which is better, SQL or Pandas?\n",
        "\n",
        "Now that you have a taste for SQL, let's try to use our new skill to query stack exchange in this notebook and put this debate to rest.\n",
        "\n",
        "**TODO**: Find all of the answers to a post that asks about Pandas vs. SQL. Here are some clues that will come in handy:\n",
        "1. This post contains the words \"pandas\" and \"sql\"\n",
        "2. This post has the most viewcount out of all the posts with both of those words\n",
        "3. The answers to this post have the column `ParentId` equal to the post's `Id`\n",
        "\n",
        "Again, no need to do this in pandas, but your answer, stored in `versus_df` will have the following format:\n",
        "\n",
        "> QuestionId | Question | QuestionBody | AnswerId | AnswerBody \n",
        ">--- | --- | --- | --- | ---\n",
        "\n",
        "SQL Hint: take a look at the `LIKE` function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ONi-nKJa2sp",
        "outputId": "9edb7139-c073-480d-f7e3-1c0bcaaac765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "versus_query = \\\n",
        "'''\n",
        "SELECT query_1.Id AS QuestionId,\n",
        "query_1.Title AS Question,\n",
        "query_1.Body as QuestionBody,\n",
        "answers_df.Id as AnswerId,\n",
        "answers_df.Body as AnswerBody\n",
        "FROM answers_df INNER JOIN \n",
        "(\n",
        "  SELECT *\n",
        "  FROM questions_df\n",
        "  WHERE Title LIKE '%pandas%' AND Title LIKE '%sql%'\n",
        "  ORDER BY questions_df.ViewCount DESC LIMIT 1\n",
        ") query_1\n",
        "ON answers_df.ParentId == query_1.Id\n",
        "'''\n",
        "versus_df = ps.sqldf(versus_query)\n",
        "versus_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QuestionId</th>\n",
              "      <th>Question</th>\n",
              "      <th>QuestionBody</th>\n",
              "      <th>AnswerId</th>\n",
              "      <th>AnswerBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34359</td>\n",
              "      <td>&lt;p&gt;First, pandas is not that much popular. I u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34366</td>\n",
              "      <td>&lt;p&gt;The real first question is why are people m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34369</td>\n",
              "      <td>&lt;p&gt;I'm one of those people who would use (in m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34375</td>\n",
              "      <td>&lt;p&gt;As much as there is overlap in the applicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34383</td>\n",
              "      <td>&lt;p&gt;The only thing not covered in these answers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>34445</td>\n",
              "      <td>&lt;p&gt;I thought I would add that I do a lot of ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>35621</td>\n",
              "      <td>&lt;h2&gt;Things Pandas can do, that SQL can't do&lt;/h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>40881</td>\n",
              "      <td>&lt;p&gt;I'll attempt to answer this question based ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>45701</td>\n",
              "      <td>&lt;p&gt;I'm fairly new to Pandas/Python but have 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>46446</td>\n",
              "      <td>&lt;p&gt;Panda is more popular since python in the f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>34357</td>\n",
              "      <td>Why do people prefer Pandas to SQL?</td>\n",
              "      <td>&lt;p&gt;I've been using SQL since 1996, so I may be...</td>\n",
              "      <td>58591</td>\n",
              "      <td>&lt;p&gt;Not exactly the answer to the question, but...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    QuestionId  ...                                         AnswerBody\n",
              "0        34357  ...  <p>First, pandas is not that much popular. I u...\n",
              "1        34357  ...  <p>The real first question is why are people m...\n",
              "2        34357  ...  <p>I'm one of those people who would use (in m...\n",
              "3        34357  ...  <p>As much as there is overlap in the applicat...\n",
              "4        34357  ...  <p>The only thing not covered in these answers...\n",
              "5        34357  ...  <p>I thought I would add that I do a lot of ti...\n",
              "6        34357  ...  <h2>Things Pandas can do, that SQL can't do</h...\n",
              "7        34357  ...  <p>I'll attempt to answer this question based ...\n",
              "8        34357  ...  <p>I'm fairly new to Pandas/Python but have 20...\n",
              "9        34357  ...  <p>Panda is more popular since python in the f...\n",
              "10       34357  ...  <p>Not exactly the answer to the question, but...\n",
              "\n",
              "[11 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18GhQqQkR-YC"
      },
      "source": [
        "We highly recommend that you read the responses! They are actually all pretty accurate and go into the pros/cons that you probably encountered while working through the problem set. Use `pd.set_option('display.max_colwidth', -1)` to view the full columns and when you're done set the colwidth back to a value like `20` so that you don't have giant dataframes in the next steps.\n",
        "\n",
        "(You could also try to find the same question via Google Search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-bUUjDR9u9"
      },
      "source": [
        "# pd.set_option('display.max_colwidth', 20)\n",
        "# versus_df"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2VBUPWLd5J0",
        "outputId": "ae8d519f-5962-4c42-f78e-4e487dc6d105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_versus_query', answer = versus_query)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxWaEdOd7ZJ",
        "outputId": "0971c5ce-9efc-4d9d-a74b-e6cf1d3456df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "grader.grade(test_case_id = 'test_versus_df', answer = versus_df)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNUhlz9ftHI"
      },
      "source": [
        "## Part 3: Working with Text Data [22 points]\n",
        "\n",
        "Shifting gears, let's now try to do some text-based analysis. Our Stack Exchange data has plenty of text that we can play with, from the user descriptions to the posts themselves. Text data is complex, but can also be used to generate extremely interpretable results, making it valuable and interesting. \n",
        "\n",
        "Throughout this section, we will attempt to answer the following:\n",
        "\n",
        "### What types of questions should I ask to get a higher reputation on Stack Exchange? \n",
        "\n",
        "Users on stack exchange are valued based on their reputation, which depends on the quality of your posts. Each post receives a score, where **score = number of upvotes - number of downvotes**. This value is already present in your posts_df. \n",
        "\n",
        "Both questions and answers get scores, but let's just focus on what types of questions we should/shouldn't ask in order to get a higher score and thus higher reputation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhKJNx74fLZx"
      },
      "source": [
        "### 3.1 Getting Highest and Lowest Scored Posts\n",
        "**TODO:** First, let's get questions with the negative scores from `questions_df` and then get the **same number** of questions with highest scores. Convert the **Body** column of the highest/lowest scorers into two lists: **highest_content** and **lowest_content**. Be sure to sort when needed!\n",
        "\n",
        "Feel free to use either **pandas** or **pandasql** to accomplish this :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Wp8Wh_oSZP"
      },
      "source": [
        "highest_content = # TODO: should be a list\n",
        "lowest_content = # TODO: should be a list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ZJHjNKfMTu"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_content', answer = lowest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWWkJMFXglNf"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_content', answer = highest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyQXqWu20mkc"
      },
      "source": [
        "### 3.2 Cleaning our Text with Regex\n",
        "Now that we have the content of our highest/lowest scored posts, we will now need to clean and tokenize them. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cCz0zh2ZSzM"
      },
      "source": [
        "First, before we do anything, let's just take a look at what we are working with\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5KJ4fYy2v9d"
      },
      "source": [
        "highest_content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wduz_X6a25lN"
      },
      "source": [
        "You probably noticed a couple of things:\n",
        "\n",
        "1. html tags (\\<p\\>, \\<a\\>, etc.)\n",
        "2. embedded latex (words surrounded $$)\n",
        "3. newline characters(\\n)\n",
        "\n",
        "We are going to clean out all of these cases using **regex**, a staple text processing tool that matches strings based on a specified pattern. Creating these patterns is actually considered a form of art to some, as the syntax is very extensive. As a brief introduction here are some basic pattern components that you will need to know:\n",
        "- \"c\": matches a \"c\" character in a string\n",
        "- \"c*?\": matches 0 or more c characters\n",
        "- \".\" matches any character\n",
        "- \".*?c\": matches any characters until you encounter \"c\"\n",
        "\n",
        "Note: the \"?\" makes the astericks less greedy and severe when removing parts of the string. It's good practice to include it, but not always necessary.\n",
        "\n",
        "**TODO:** Below, create a function **remove_bad_patterns(text)** that removes all of the 3 cases listed above from a given string, text. You will need to \n",
        "1. create patterns to handle each of the cases\n",
        "2. use **re.sub(pattern, newstring)** to substitute all matches with the empty string, \"\". If you want to test your pattern, check out [this tool](https://regexr.com).\n",
        "\n",
        "Note: \"$\" is considered a special character in regex, so you will need to escape it with \"\\\\$\" to specify you want to match the character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RddxrRuFsWoh"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_bad_patterns(text):\n",
        "    \"\"\"Remove html, latex, and newline characters from a string\n",
        "    \n",
        "    :param text: content as a string\n",
        "    :return: cleaned text string\n",
        "    \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KuHZkyfMjHB"
      },
      "source": [
        "Now, apply this function to both **highest_content** and **lowest_content** to create **cleaned_highest_content** and **cleaned_lowest_content**, respectively, and let's take another look at the new and improved first entry:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHv_uU6-L5kL"
      },
      "source": [
        "cleaned_highest_content = # TODO\n",
        "cleaned_lowest_content =  # TODO\n",
        "cleaned_highest_content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7VMJsGZh9Ym"
      },
      "source": [
        "grader.grade(test_case_id = 'test_cleaned_highest', answer = cleaned_highest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqa35Zyhh9jH"
      },
      "source": [
        "grader.grade(test_case_id = 'test_cleaned_lowest', answer = cleaned_lowest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0qh7TRMoj7"
      },
      "source": [
        "A lot cleaner, right? Of course, it's not perfect but it'll do for our purposes in this homework. With that out of the way let us now...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwLCBeTOLy3"
      },
      "source": [
        "###3.3 Tokenize the Text\n",
        "\n",
        "Here, we are going to split up the content into a list of words. Here, we will use the **nltk** package, which contains an extensive set of tools to process text. Of course, like regex, this homework would be miles long if we really went into detail, so we are only going to utilize the following components:\n",
        "- nltk.word_tokenize(): a function used to tokenize our text\n",
        "- nltk.corpus.stopwords: a list of commonly used words such as \"a\",\"an\",\"in\" that are often ignored in text-related analysis\n",
        "\n",
        "\n",
        "**TODO:** First, use **stopwords** to create a set of the most common english stopwords. Then, implement **tokenized_content(content)** that takes in a content string and \n",
        "1. tokenizes the text\n",
        "2. lowercases the token\n",
        "3. removes stop words (commonly used words such as \"a\",\"an\", \"in\")]\n",
        "4. keeps words with only alphabet characters (no punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWiVzUUpvjA"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEwEm-E7pxjT"
      },
      "source": [
        "stopwords = set(stopwords.words('english')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbi4KvlXpJXV"
      },
      "source": [
        "def tokenize_content(content):\n",
        "  \"\"\"returns tokenized string\n",
        "\n",
        "  :param content: text string\n",
        "  :return: tokenized text/list of words\n",
        "  \"\"\"\n",
        "  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es5zPleHSROw"
      },
      "source": [
        "Now, apply your tokenized_titles function to each piece of content in **cleaned_highest_content** and **cleaned_lowest content** and flatten both of the lists to create **highest_tokens** and **lowest_tokens**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsxzxdYpiic"
      },
      "source": [
        "highest_tokens = # TODO\n",
        "lowest_tokens =  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4v6dzmXk6Yp"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_tokens', answer = highest_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kPhea-Tk9Nh"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_tokens', answer = lowest_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUYScz9YSKsM"
      },
      "source": [
        "### 3.4 Most Frequent Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0sTQHvp5E6"
      },
      "source": [
        "Now, find the 20 most common words amongst the content of your highest and lowest questions.\n",
        "\n",
        "\n",
        "\n",
        "Hint: https://docs.python.org/2/library/collections.html#counter-objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRWGYkoep-I3"
      },
      "source": [
        "lowest_counter = # TODO\n",
        "lowest_most_common = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vusua1Edp8mm"
      },
      "source": [
        "highest_counter = # TODO\n",
        "highest_most_common = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7WeWoRZtEOT"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_most_common', answer = highest_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG2W2YIbuJQj"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_most_common', answer = lowest_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvwIdmhNZmhY"
      },
      "source": [
        "###3.5 Refining our Lists\n",
        "\n",
        "Hmmm...both of these lists seem to overrepresent the common jargon of data science. Let's try to tease out words that distinguish the high from the low scoring posts. \n",
        "\n",
        "One approach would be to find words in one list that are not in the other. This, however, may be too naive, as even if a word is extremely common in our high list, if it appears only once in our low list, it would get removed from consideration.\n",
        "\n",
        "Let's instead find the difference between the counts within our two lists. With this method, if a word is really common in one, but not the other, the count would only decrease slightly. Alternatively, if a word is common in both lists, it would effectively zero out.\n",
        "\n",
        "**TODO:** Using the difference method, create a **distinct_highest_common** and **distinct_lowest_commonr**  that find the top 20 counts of words within each group of posts after using the difference method described above. Be careful on which list you are subtracting!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnx8MsbBA99U"
      },
      "source": [
        "distinct_highest_common = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT3hzB_vA2UA"
      },
      "source": [
        "distinct_lowest_common = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJZutqmMwLoR"
      },
      "source": [
        "grader.grade(test_case_id = 'test_distinct_highest_common', answer = distinct_highest_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCCFeH6-wMQS"
      },
      "source": [
        "grader.grade(test_case_id = 'test_distinct_lowest_common', answer = distinct_lowest_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztARzQHyexXQ"
      },
      "source": [
        "The lists are much more different right? It seems as if low scoring posts tend to ask a lot about errors/code while higher posts are much more conceptual based.\n",
        "\n",
        "So if you're a looking for a high reputation, don't ask people to debug your code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEAi4a6OcjE4"
      },
      "source": [
        "### 3.6 Word Clouds\n",
        "\n",
        "Before we move on from this dataset, let's do one final step and visualize our results with wordclouds.\n",
        "\n",
        "**TODO**: Take a look at [this documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) and create two word clouds for our two groups of distinct tokens.\n",
        "\n",
        "Be sure to create these on the full list of distinct tokens, and not just the top 20. We will be going through your notebooks and manually grading your world clouds (worth 4 points). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYNzQJDicpLw"
      },
      "source": [
        "highest_wordcloud = # TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONjr3Kq1ciCu"
      },
      "source": [
        "lowest_wordcloud = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISe_XMH7ivEa"
      },
      "source": [
        "#Section 2: Spark, Hierarchical Data and Graph Data on Yelp Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJHiaJ_1P9G"
      },
      "source": [
        "## Getting Started with Apache Spark\n",
        "\n",
        "Now that you've seen how to run SQL queries through pandas, we'll working with running SQL in Apache Spark! Apache Spark is a complex, cluster-based data processing system written in Scala used for big data processing. For the most part, Spark interfaces smoothly to Python.\n",
        "\n",
        "While Spark dataframes try to emulate the same programming style as Pandas DataFrames, there are some differences in how you express things. Please refer to the Lecture Slides or the following resources to learn about these differences:\n",
        "\n",
        "https://lab.getbase.com/pandarize-spark-dataframes/\n",
        "https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/ \n",
        "\n",
        "For this assignment, we are going to get familiar with Spark without worrying too much about sharding and distribution. This isnt really using it to its strengths -- and in fact you might find Spark to be slow -- but it will get you comfortable with programming in Spark without worrying about distributed nodes, clusters, and spending real dollars on the cloud. For Homework 3, well connect your Jupyter instance to Spark running on the cloud.\n",
        "\n",
        "### Initializing a Connection to Spark\n",
        "\n",
        "We'll open a connection to Spark as follows. From `SparkSession`, you can load data into Spark DataFrames as well as `RDD`s.\n",
        "\n",
        "Run the following cells to setup this part of the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8RH4R771X6n"
      },
      "source": [
        "%%capture\n",
        "!apt install libkrb5-dev\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "! pip install pyspark --user\n",
        "! pip install seaborn --user\n",
        "! pip install plotly --user\n",
        "! pip install imageio --user\n",
        "! pip install folium --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNctzcXRkexY"
      },
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP28kxLekWG7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gm6aXPq1Ulc"
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGROEgu1gfN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "# misc\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "\n",
        "# graph section\n",
        "import networkx as nx\n",
        "# import heapq  # for getting top n number of things from list,dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "\n",
        "# Time conversions\n",
        "import time\n",
        "\n",
        "# Parallel processing\n",
        "# import swifter\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF0xipwC1hme"
      },
      "source": [
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-YbXpG1owp"
      },
      "source": [
        "### Download data\n",
        "\n",
        "The following code retrieves the Yelp dataset files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuRm7t0it3nF"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1XCANGSCd0pUNcXq18t2QDwCIpJxus8Dy',\n",
        "                                    dest_path='/content/yelp_business_attributes.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='11lwBibxX7PYGgOfHU25_dDDDsPX1Pt0Y',\n",
        "                                    dest_path='/content/yelp_business.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1FU5Q-96erhTmk8SjC4XHUm94yWc6h3a0',\n",
        "                                    dest_path='/content/yelp_checkin.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1UaaLrCKjqoQ7G3JT_VUw56pc-dnTwyrS', dest_path='/content/yelp_review2.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1JNFZeLlimxNSwcOb-oBxxbwJqdg22WgD',\n",
        "                                    dest_path='/content/yelp_user.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srYXW3JwvIZi"
      },
      "source": [
        "## Part 4: Working with Spark [21 points total]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tjhGYPK1vmm"
      },
      "source": [
        "### 4.1 Load Our Datasets\n",
        "\n",
        "\n",
        "In this section, we'll be using Spark to look into social data from Yelp. To start, let's read our data into Spark. As an example of how to do this, to load the file `input.txt` into a Spark DataFrame, you can use lines like the following.\n",
        "\n",
        "```\n",
        "# Read lines from the text file\n",
        "input_sdf = spark.read.load('input.txt', format=\"text\")\n",
        "```\n",
        "\n",
        "Well use the suffix `_sdf` to represent Spark DataFrame, much as we used `_df` to denote a Pandas DataFrame. \n",
        "\n",
        "\n",
        "**TODO:** Load the various files from Yelp. Your datasets should be named `yelp_business_sdf`, `yelp_business_attributes_sdf`, `yelp_check_in_sdf`, `yelp_reviews_sdf`, and `yelp_users_sdf`. Submit the first 75 entries of the yelp_business_sdf, sorted by the \"name\" column in ascending order, to the autograder as a pandas dataframe by using the toPandas() function to convert it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mtQl7Bi1rHe"
      },
      "source": [
        "# TODO: load Yelp datasets\n",
        "\n",
        "yelp_business_sdf = # TODO\n",
        "yelp_business_attributes_sdf = # TODO \n",
        "yelp_check_in_sdf = # TODO \n",
        "yelp_reviews_sdf = # TODO\n",
        "yelp_users_sdf = # TODO \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRjwuThgjdze"
      },
      "source": [
        "yelp_business = yelp_business_sdf.toPandas().sort_values(by = ['name'], ascending = True)[:75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqHKFiT-12wT"
      },
      "source": [
        "grader.grade(test_case_id = 'check_yelp_load', answer = yelp_business[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvm3tmQ7c6N-"
      },
      "source": [
        "One key difference between using Pandas SQL and Spark SQL is that you'll need to create a view of your data before Spark is able to query it. Note that when using a temporary view as we will be doing, Spark does not persist the data in memory. \n",
        "\n",
        "**TODO:** Put all of your Spark dataframes from the previous section into temporary tables. The syntax is as follows:\n",
        "*yelp_business_sdf.createOrReplaceTempView('yelp_business')*\n",
        "\n",
        "Note that when you're accessing the yelp data within Spark SQL later in the homework you'll want to refer to each table as the name of the view you assigned - for instance \"yelp_business\" not \"yelp_business_sdf\". This distinction is important as this table is only visible to Spark as \"yelp_business\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADFIa6jx16Sm"
      },
      "source": [
        "# TODO: save tables with names such as yelp_business, yelp_users\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-C8wBruelo4"
      },
      "source": [
        "Next, explore the sdf's using the Sandbox area below. Some functions that might be useful are:\n",
        "* show (shows the first few rows of data, similar to head in Pandas)\n",
        "* count (counts number of rows, similar to using len in Pandas)\n",
        "* dtypes (same as in Pandas) \n",
        "* describe (use with show to see summary statistics, similar to just describe on its own in Pandas)\n",
        "\n",
        "You are not limited to these functions and are welcome to use any other ones. The purpose of this exploration is to get a sense of what the data looks like before moving on. Again, **this section is not graded**, but we encourage you to get familiar with the data before diving in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc4VrDtToDVB"
      },
      "source": [
        "# your EDA here! feel free to add more cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBjwu_4ZbgdG"
      },
      "source": [
        "### 4.2 Simple Analytics on the Data\n",
        "In this section, we will be executing Spark operations on the data given. Beyond simply executing the queries, you may try using .explain() method to see more about the query execution. Also, please read the data description prior to attempting the following questions to understand the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S-K77HvqCJi"
      },
      "source": [
        "#### 4.2.1 Spark and Big Data\n",
        "You may be wondering why we can't just use Pandas SQL for analytics on the yelp data. As the data we're working with gets larger data, performance in Pandas will slow - or the data may even be too large to load into Pandas.\n",
        "\n",
        "For a simple example, let's compare how long the same query takes to run in Pandas SQL and Spark SQL.\n",
        "\n",
        "**TODO:** First, convert the yelp business table to Pandas. Then, using the yelp business table, select the name of businesses located in Pennsylvania. Run this query in both Pandas SQL and Spark SQL and time how long the query takes to run. The time module will be useful for this. You may want to separate your code into several cells to ensure you are only timing one query at a time. Submit the ratio of the time it took the query to run in Pandas SQL to the time it took the query to run in Spark SQL, called `time_ratio`, to the autograder.\n",
        "\n",
        "As a reminder, Spark uses lazy computation, meaning results are delayed until they are actually needed. Therefore, you will need to show your table (or do some other computation that requires the table to be generated) in order for your query to run in Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6i-3fPdTKxH"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljPlRHFAlR35"
      },
      "source": [
        "# TODO: Convert the yelp_business_sdf to Pandas \n",
        "yelp_business_df = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adEbp1mhqKKR"
      },
      "source": [
        "# TODO: Time the query takes to run in Pandas SQL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df-qacPRjsny"
      },
      "source": [
        "# TODO: Time the query takes to run in Spark SQL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vABMmUcwl6xS"
      },
      "source": [
        "# TODO: Ratio of time taken in Pandas SQL to time taken in SPark SQL\n",
        "\n",
        "time_ratio = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xWd4MJr6sr"
      },
      "source": [
        "grader.grade(test_case_id = 'time_check', answer = time_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6n8ooy9iIqH"
      },
      "source": [
        "#### 4.2.2 Cities by number of businesses\n",
        "\n",
        "Next, we'll explore which cities have the most restaurants. \n",
        "\n",
        "**TODO:** Find the top 10 cities by number of (Yelp-listed) businesses. This table should include `city`, `state`, and `num_restaurants`, which is the number of restaurants in the city. Convert this sdf to Pandas and submit top10_cities_df to the autograder. Remember to only convert small tables to Pandas!\n",
        "\n",
        "\n",
        "Your table should look something like:\n",
        ">city | state | num_restaurants\n",
        ">--- | --- |--- \n",
        ">city 1 | state 1|  rating 1 | number 1\n",
        ">city 2| state 2| rating 2 | number 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7lPtnowiH5u"
      },
      "source": [
        "# TODO: cities by number of businesses\n",
        "# Worth 5 points\n",
        "\n",
        "top10_cities_df = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRDb7UcThGDE"
      },
      "source": [
        "grader.grade(test_case_id = 'top10CitiesCheck', answer = top10_cities_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L8pEamDbeUC"
      },
      "source": [
        "#### 4.2.3 Business ratings across states\n",
        "\n",
        "Next, we'll be looking into how ratings for the same business vary state by state. Throughout this problem, we'll be intersted in the *average rating by business and by state*. \n",
        "\n",
        "**TODO:** For each business, find the states where the business's average rating is below the *maximum of the business's per-state* average rating.  Think about how to factor that into steps!\n",
        "\n",
        "* Compute the average rating for each business name by state. For each business, find the maximum average rating across all states' average ratings. \n",
        "\n",
        "* Then compute an sdf containing the business name, state, avg_rating, and max_avg_rating for businesses in states where that business is *not* most highly rated. Order the output in order of business name, decreasing avg_rating, and increasing state name. \n",
        "\n",
        "Convert the top 100 rows to Pandas and submit `below_avg_states_df` to the autograder.\n",
        "\n",
        "Your table should look something like:\n",
        ">name | state |avg_rating | max_avg_rating\n",
        ">--- | --- |--- | ---\n",
        ">business name 1 | state 1|  rating 1 | maxing rating 1\n",
        ">business name 1 | state 2| rating 2 | maxing rating 1\n",
        ">business name 2 | state 3| rating 3 | maxing rating 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X8U1EQebds6"
      },
      "source": [
        "below_avg_states_df = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpoocHBtfyrB"
      },
      "source": [
        "grader.grade(test_case_id = 'check_by_state_rating', answer = below_avg_states_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7auunIQG7hi2"
      },
      "source": [
        "###4.3 Format Yelp Data as a Graph\n",
        "\n",
        "\n",
        "The Yelp data you've been working with can be thought of as graph data. Recall that a graph is made up of a set of verticies that are connected by edges. Within the context of our data, we can think of the users/businesses as nodes. Edges would then represent a review by a user for a business.\n",
        "\n",
        "With this in mind, we now want to reformat the yelp_reviews dataset to look more like a graph. \n",
        "\n",
        "**TODO:** Use Spark SQL to rename the user_id column of yelp_reviews data to from_node and rename the business_id column to to_node. Filter to rows where both the user_id and business_id are not null.  Create a temporary view with this table.\n",
        "\n",
        "Your table should look something like:\n",
        ">from_node | to_node | score\n",
        ">--- | --- | ---\n",
        ">user id 1 | business id 1 | stars 1\n",
        ">user id 2 | business id 2 | stars 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlSq4FCbbCh"
      },
      "source": [
        "review_graph_sdf = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36o1clK3jwzY"
      },
      "source": [
        "review_graph_sdf.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qz2_SEHYztC"
      },
      "source": [
        "**TODO:** Once you've made your graph and created a temporary view, use Spark SQL to filter to the rows in the graph that contain the sequence \"abc\" anywhere in the from_node. Convert this subset to a Pandas dataframe called named `review_graph_abc`  and submit this to the autograder.\n",
        "\n",
        "HINT: Look into the LIKE keyword and wildcards in SQL. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWBs7KXW9Bo"
      },
      "source": [
        "review_graph_abc_sdf = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joCY97gtaiJi"
      },
      "source": [
        "# Add test case for making graph\n",
        "grader.grade(test_case_id = 'reviewGraphCheck', answer = review_graph_abc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA84jsFau2Ls"
      },
      "source": [
        "\n",
        "## Part 5. Traversing a Graph [21 points total]\n",
        "\n",
        "For our next tasks, we will be walking the graph and making connections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrftJPvUiY59"
      },
      "source": [
        "\n",
        "### 5.1 Intro to Distributed Breadth-First Search\n",
        "\n",
        "\n",
        "Now that we have created our graph, we will be implementing a graph traversal algorithm known as Breadth First Search. It works in a way that's equivalent to how a stain spreads on a white t-shirt. Take a look at the graph below:\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src = \"https://imgur.com/WU3AUwg.png\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "* Consider starting BFS from point A (green). This is considered the starting frontier/singular origin node.\n",
        "* The first round of BFS would involve finding all the nodes directly reachable from A, namely B-F (blue circles). These blue nodes make up the next frontier at depth 1 away from our starting node A.\n",
        "* The second round would then be identifying the red nodes which are the neighbors of the blue nodes. Now, the red nodes all belong to a frontier 2 depth away from A.\n",
        "\n",
        "This process continues until all the nodes in the graph have been visited. \n",
        "\n",
        "\n",
        "If you would like to learn more about BFS, I highly suggest looking [here](https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.html).\n",
        "\n",
        "\n",
        "We will now be implementing **spark_bfs(G, N, d)**, our spark flavor of BFS that takes a graph **G**, a set of origin nodes **N**, and a max depth **d**.\n",
        "\n",
        "In order to write a successful BFS function, you are going to need to figure out \n",
        "1. how to keep track of nodes that we have visited\n",
        "2. how to properly find all the nodes at the next depth\n",
        "3. how to avoid cycles and ensure that we do not constantly loop through the same edges (take a look at J-K in the graph)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG8yQrCLjegX"
      },
      "source": [
        "### 5.2 Implement one Traversal\n",
        "\n",
        "To break down this process, let's think about how we would implement a single traversal of the graph. That is given the green node in the graph above, how are we going to get the blue nodes?\n",
        "\n",
        "\n",
        "Consider the simple graph below **which is different from the graph in the image above**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uQoYThcBSdZ"
      },
      "source": [
        "simple = [('A', 'B'),\n",
        "         ('A', 'C'),\n",
        "         ('A', 'D'),\n",
        "         ('C', 'F'),\n",
        "         ('F', 'A'),\n",
        "         ('B', 'G'),\n",
        "         ('G', 'H'),\n",
        "         ('D', 'E')]\n",
        "simple_dict = {'from_node': ['A', 'A', 'A', 'C', 'F', 'B', 'G', 'D'],\n",
        "       'to_node': ['B', 'C', 'D', 'F', 'A', 'G', 'H', 'E']}\n",
        "simple_graph_df = pd.DataFrame.from_dict(simple_dict)\n",
        "simple_graph_sdf = spark.createDataFrame(simple_graph_df)\n",
        "simple_graph_sdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHRMuWzgqykO"
      },
      "source": [
        "As you can see, each row of this dataframe represents an edge between two nodes Although the nodes are labeled \"from\" and \"to\", the edges are actually undirected, meaning that A-->B represents the same edge as B-->A.\n",
        "\n",
        "Let's define our starting node as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcs0x5KaJi_B"
      },
      "source": [
        "smallOrig = [{'node': 'A'}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhTWrTrvJRJ"
      },
      "source": [
        "Then, bfs with graph G, starting from smallOrig to depth 1, or  **spark_bfs(G, smallOrig, 1)** would output as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGq2lYQKvJ0S"
      },
      "source": [
        "simple_1_round_dict = {'node': ['F', 'B', 'D', 'C', 'A'],\n",
        "       'distance': [1, 1, 1, 1, 0]}\n",
        "simple_1_round_bfs_df = pd.DataFrame.from_dict(simple_1_round_dict)\n",
        "simple_1_round_bfs_sdf = spark.createDataFrame(simple_1_round_bfs_df)\n",
        "simple_1_round_bfs_sdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjF7XBz9vcDD"
      },
      "source": [
        "As you can see, this dataframe logs each node with its corresponding distance away from A. Moreover, we also know that these nodes are **visited**. \n",
        "\n",
        "Hopefully, you can see how we can use our original graph and this new information to find the nodes at depth two. \n",
        "\n",
        "This is exactly what we will try to accomplish with **spark_bfs_1_round(visited_nodes)** which will ultimately be the inner function of **spark_bfs** that we use to perform exactly one traversal of a graph.\n",
        "\n",
        "**TODO**: Write **spark_bfs_1_round(visted_nodes)** that takes the currently dataframe of visited_nodes, performs one round of BFS, and returns an updated visited nodes dataframe. You should assume that a temporary sdf G already exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yz3Gz5FAtrW"
      },
      "source": [
        "def spark_bfs_1_round(visited_nodes):\n",
        "  \"\"\"\n",
        "  :param visited_nodes: dataframe with columns node and distance\n",
        "  :return: dataframe of updated visuted nodes, with columns node and distance\n",
        "  \"\"\"\n",
        "  \n",
        "  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1LRfM4NMvV"
      },
      "source": [
        "Now, run the inner function on **simple_1_round_bfs_sdf** result of 1 round of BFS on simple graph and store the results in **simple_bfs_result**. This is ultimately what the output of BFS to depth 2 should look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDboxYrZKH-l"
      },
      "source": [
        "simple_graph_sdf.createOrReplaceTempView('G')\n",
        "simple_bfs_result = # TODO\n",
        "simple_bfs_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD82GqyJNWGu"
      },
      "source": [
        "Convert this result to Pandas, sorted by the node, and submit it to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_vnk78_K9B1"
      },
      "source": [
        "simple_bfs_test = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKcVOdEdLOXZ"
      },
      "source": [
        "grader.grade(test_case_id = 'checksimpleBFS', answer = simple_bfs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOnnxL65yssC"
      },
      "source": [
        "### 5.3 Full BFS Implemntation\n",
        "\n",
        "Now, we will fully implement **spark_bfs**. This function should iteratively call your implemented version of **spark_bfs_1_round** and ultimately return the output of this function at **max_depth**.\n",
        "\n",
        "You are also responsible for initializing the starting dataframe, that is converting the list of origin nodes into a spark dataframe with the nodes logged at distance 0.\n",
        "\n",
        "Consider the following: \n",
        "\n",
        "```\n",
        "schema = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "    my_sdf = spark.createDataFrame(origins, schema)\n",
        "```\n",
        "\n",
        "The schema ultimately specifies the structure of the Spark DataFrame with a string `node` column. It then calls **spark.createDataFrame** to map this schema to the **origins** nodes. Also, you are responsible for ensuring that a view of your graph is available within this function. (Note: you will also need to add in a distance column)\n",
        "\n",
        "**TODO:** implement **spark_bfs(G,origins,max_depth)** and run on **review_graph_sdf** initalized in 4.3. Note: you may want to run tests on the **simple_graph** example as the `review_graph_sdf` will take quite some time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXkRvJXKiVum"
      },
      "source": [
        "# TODO: iterative search over undirected graph\n",
        "# Worth 5 points directly, but will be needed later\n",
        "\n",
        "def spark_bfs(G, origins, max_depth):\n",
        "  \"\"\" runs distributed BFS to a specified max depth\n",
        "\n",
        "  :param G: graph dataframe from 4.3\n",
        "  :param origins: list of origin nodes stored as {\"node\": nodeValue}\n",
        "  :param max_depth: integer value of max depth to run BFS to\n",
        "  :return: dataframe with columns node, distance of all visited nodes\n",
        "  \"\"\"\n",
        "\n",
        "  # TODO \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFA-Al0MNutj"
      },
      "source": [
        "Test that this function works on the simple example first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twqNOYGOM_5u"
      },
      "source": [
        "simple_bfs_iterative_result = spark_bfs(simple_graph_sdf, smallOrig, 3)\n",
        "simple_bfs_iterative_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYpE6Pp1njJu"
      },
      "source": [
        "**TODO**: Using the starting node defined below, create **bfs_3** as the result of running **sparkbfs** on **review_graph_sdf** to a depth of 3. Finally, create a pandas dataframe of the first 75 results sorted by id as **answer_75_df** and submit this to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUwgLmhiiAY"
      },
      "source": [
        "orig = [{'node': 'bv2nCi5Qv5vroFiqKGopiw'}]\n",
        "# grab the count\n",
        "bfs_3 = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZwiin6fnV8d"
      },
      "source": [
        "answer_75_df = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZnMt_CKnouC"
      },
      "source": [
        "When submitting to the autograder, submit as a tuple where first value is the length of your output dataframe and the second is the first 75 rows of your result.\n",
        "\n",
        "However, before you grab your first 75 rows, sort by the ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAqdWoVtjDMI"
      },
      "source": [
        "# 13603 is just obtained from running count.count()\n",
        "grader.grade(test_case_id = 'checkBFS', answer = (length, answer_75_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgwWGKJR2h7C"
      },
      "source": [
        "Congratulations on making it to the end of Homework 2! Feel free to fill out [this form](https://forms.gle/DbDuEbqqifoFrRxaA) with any feedback for this and prior homeworks. We know this assignment was pretty dense, but we hope that you still managed to learn a lot from it :)\n"
      ]
    }
  ]
}